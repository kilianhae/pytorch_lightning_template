Global seed set to 42
GPU available: True (mps), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.
  rank_zero_warn(
/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:189: UserWarning: .fit(ckpt_path="last") is set, but there is no last checkpoint available. No checkpoint will be loaded.
  rank_zero_warn(
  | Name | Type | Params
------------------------------
0 | net  | MLP  | 66.4 K
------------------------------
66.4 K    Trainable params
0         Non-trainable params
66.4 K    Total params
0.266     Total estimated model params size (MB)
/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Traceback (most recent call last):
  File "/Users/kilianhaefeli/pytorch_lightning_template/train_Points_MLP.py", line 22, in <module>
    trainer.train()
  File "/Users/kilianhaefeli/pytorch_lightning_template/project/train.py", line 143, in train
    self.trainer.fit(self.model, self.train_loader, self.validation_loader, ckpt_path = "last")
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 531, in fit
    call._call_and_handle_interrupt(
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 570, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 975, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1018, in _run_stage
    self.fit_loop.run()
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py", line 202, in run
    self.on_advance_end()
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py", line 369, in on_advance_end
    call._call_callback_hooks(trainer, "on_train_epoch_end", monitoring_callbacks=True)
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 189, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 303, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/Users/kilianhaefeli/miniconda3/envs/pytorch_lightning_template/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py", line 358, in _save_topk_checkpoint
    raise MisconfigurationException(m)
lightning.fabric.utilities.exceptions.MisconfigurationException: `ModelCheckpoint(monitor='train_epoch_total_loss')` could not find the monitored key in the returned metrics: ['train_MSELoss', 'train_L1Loss', 'val_MSELoss', 'val_epoch_MSELoss', 'val_L1Loss', 'val_epoch_L1Loss', 'train_epoch_MSELoss', 'train_epoch_L1Loss', 'epoch', 'step']. HINT: Did you call `log('train_epoch_total_loss', value)` in the `LightningModule`?
Sanity Checking DataLoader 0:   0%|                                                                                                              | 0/2 [00:00<?, ?it/s]torch.float32
torch.float32
Sanity Checking DataLoader 0:  50%|███████████████████████████████████████████████████                                                   | 1/2 [00:00<00:00, 28.22it/s]torch.float32
torch.float32
Epoch 0:   0%|                                                                                                                                  | 0/25 [00:00<?, ?it/s]torch.float32
torch.float32
Epoch 0:   4%|██▊                                                                  | 1/25 [00:00<00:00, 24.34it/s, v_num=091y, train_MSELoss=0.521, train_L1Loss=0.677]torch.float32
torch.float32
Epoch 0:   8%|█████▌                                                               | 2/25 [00:00<00:00, 45.32it/s, v_num=091y, train_MSELoss=0.426, train_L1Loss=0.558]torch.float32
torch.float32
Epoch 0:  12%|████████▎                                                            | 3/25 [00:00<00:00, 64.02it/s, v_num=091y, train_MSELoss=0.463, train_L1Loss=0.607]torch.float32
torch.float32
Epoch 0:  16%|███████████                                                          | 4/25 [00:00<00:00, 81.40it/s, v_num=091y, train_MSELoss=0.435, train_L1Loss=0.595]torch.float32
torch.float32
Epoch 0:  20%|█████████████▊                                                       | 5/25 [00:00<00:00, 97.19it/s, v_num=091y, train_MSELoss=0.567, train_L1Loss=0.707]torch.float32
torch.float32
Epoch 0:  24%|████████████████▎                                                   | 6/25 [00:00<00:00, 109.34it/s, v_num=091y, train_MSELoss=0.455, train_L1Loss=0.593]torch.float32
torch.float32
Epoch 0:  28%|███████████████████                                                 | 7/25 [00:00<00:00, 116.25it/s, v_num=091y, train_MSELoss=0.533, train_L1Loss=0.668]torch.float32
torch.float32
Epoch 0:  32%|█████████████████████▊                                              | 8/25 [00:00<00:00, 126.46it/s, v_num=091y, train_MSELoss=0.439, train_L1Loss=0.582]torch.float32
torch.float32
Epoch 0:  36%|████████████████████████▍                                           | 9/25 [00:00<00:00, 124.48it/s, v_num=091y, train_MSELoss=0.497, train_L1Loss=0.623]torch.float32
torch.float32
Epoch 0:  40%|██████████████████████████▊                                        | 10/25 [00:00<00:00, 132.33it/s, v_num=091y, train_MSELoss=0.421, train_L1Loss=0.563]torch.float32
torch.float32
Epoch 0:  44%|█████████████████████████████▍                                     | 11/25 [00:00<00:00, 140.44it/s, v_num=091y, train_MSELoss=0.362, train_L1Loss=0.527]torch.float32
torch.float32
Epoch 0:  48%|████████████████████████████████▏                                  | 12/25 [00:00<00:00, 146.86it/s, v_num=091y, train_MSELoss=0.506, train_L1Loss=0.642]torch.float32
torch.float32
Epoch 0:  52%|██████████████████████████████████▊                                | 13/25 [00:00<00:00, 153.38it/s, v_num=091y, train_MSELoss=0.482, train_L1Loss=0.634]torch.float32
torch.float32
Epoch 0:  56%|█████████████████████████████████████▌                             | 14/25 [00:00<00:00, 160.06it/s, v_num=091y, train_MSELoss=0.440, train_L1Loss=0.585]torch.float32
torch.float32
Epoch 0:  60%|████████████████████████████████████████▏                          | 15/25 [00:00<00:00, 166.67it/s, v_num=091y, train_MSELoss=0.436, train_L1Loss=0.595]torch.float32
torch.float32
Epoch 0:  64%|██████████████████████████████████████████▉                        | 16/25 [00:00<00:00, 171.96it/s, v_num=091y, train_MSELoss=0.420, train_L1Loss=0.583]torch.float32
torch.float32
Epoch 0:  68%|█████████████████████████████████████████████▌                     | 17/25 [00:00<00:00, 176.24it/s, v_num=091y, train_MSELoss=0.432, train_L1Loss=0.590]torch.float32
torch.float32
Epoch 0:  72%|████████████████████████████████████████████████▏                  | 18/25 [00:00<00:00, 172.23it/s, v_num=091y, train_MSELoss=0.413, train_L1Loss=0.589]torch.float32
torch.float32
Epoch 0:  76%|██████████████████████████████████████████████████▉                | 19/25 [00:00<00:00, 176.82it/s, v_num=091y, train_MSELoss=0.505, train_L1Loss=0.659]torch.float32
torch.float32
Epoch 0:  80%|█████████████████████████████████████████████████████▌             | 20/25 [00:00<00:00, 180.61it/s, v_num=091y, train_MSELoss=0.505, train_L1Loss=0.667]torch.float32
torch.float32
Epoch 0:  84%|████████████████████████████████████████████████████████▎          | 21/25 [00:00<00:00, 185.21it/s, v_num=091y, train_MSELoss=0.415, train_L1Loss=0.592]torch.float32
torch.float32
Epoch 0:  88%|██████████████████████████████████████████████████████████▉        | 22/25 [00:00<00:00, 189.47it/s, v_num=091y, train_MSELoss=0.447, train_L1Loss=0.574]torch.float32
torch.float32
Epoch 0:  92%|█████████████████████████████████████████████████████████████▋     | 23/25 [00:00<00:00, 193.36it/s, v_num=091y, train_MSELoss=0.438, train_L1Loss=0.590]torch.float32
torch.float32
Epoch 0:  96%|████████████████████████████████████████████████████████████████▎  | 24/25 [00:00<00:00, 197.76it/s, v_num=091y, train_MSELoss=0.257, train_L1Loss=0.432]torch.float32
torch.float32
Epoch 0: 100%|███████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 199.81it/s, v_num=091y, train_MSELoss=0.358, train_L1Loss=0.511]torch.float32
torch.float32taLoader 0:   0%|                                                                                                                   | 0/7 [00:00<?, ?it/s]
                                                                                                                                                                       torch.float32
torch.float32taLoader 0:  14%|███████████████▏                                                                                          | 1/7 [00:00<00:00, 371.90it/s]
                                                                                                                                                                       torch.float32
torch.float32taLoader 0:  29%|██████████████████████████████▎                                                                           | 2/7 [00:00<00:00, 385.29it/s]
                                                                                                                                                                       torch.float32
torch.float32taLoader 0:  43%|█████████████████████████████████████████████▍                                                            | 3/7 [00:00<00:00, 399.15it/s]
                                                                                                                                                                       torch.float32
torch.float32taLoader 0:  57%|████████████████████████████████████████████████████████████▌                                             | 4/7 [00:00<00:00, 348.83it/s]
                                                                                                                                                                       torch.float32
torch.float32taLoader 0:  71%|███████████████████████████████████████████████████████████████████████████▋                              | 5/7 [00:00<00:00, 360.80it/s]
                                                                                                                                                                       torch.float32
torch.float32taLoader 0:  86%|██████████████████████████████████████████████████████████████████████████████████████████▊               | 6/7 [00:00<00:00, 376.15it/s]
Epoch 0: 100%|█| 25/25 [00:00<00:00, 167.75it/s, v_num=091y, train_MSELoss=0.358, train_L1Loss=0.511, val_MSELoss=0.376, val_epoch_MSELoss=0.376, val_L1Loss=0.534, val